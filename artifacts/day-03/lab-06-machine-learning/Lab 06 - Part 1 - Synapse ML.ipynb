{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Synapse ML into the Apache Spark session"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"name\": \"synapseml\",\n",
        "  \"conf\": {\n",
        "      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.9.4\",\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12\",\n",
        "      \"spark.yarn.user.classpath.first\": \"true\"\n",
        "  }\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detect entities in text using the Cognitive Services entity detector transformer from Synapse ML"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve the Cognitive Services credentials and create the test dataset."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = mssparkutils.credentials.getSecret('#KEY_VAULT_NAME#', '#COGNITIVE_SERVICES_SECRET_NAME#')\n",
        "location = 'northeurope'\n",
        "\n",
        "df = spark.createDataFrame(data=[\n",
        "        [1, \"Muad'Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn. It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult. Muad'Dib knew that every experience carries its lesson.\"],\n",
        "        [2, \"It’s the ship that made the Kessel run in less than twelve parsecs. I’ve outrun Imperial starships. Not the local bulk cruisers, mind you. I’m talking about the big Corellian ships, now. She’s fast enough for you, old man.\"]\n",
        "    ], \n",
        "    schema=[\"id\",\"text\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the transformer and detect the entities mentioned in text."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from synapse.ml.cognitive import *\n",
        "\n",
        "entity = (EntityDetector()\n",
        "      .setSubscriptionKey(key)\n",
        "      .setLocation(location)\n",
        "      .setLanguage(\"en\")\n",
        "      .setOutputCol(\"entities\")\n",
        "      .setErrorCol(\"error\"))\n",
        "\n",
        "df_entities = entity.transform(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the entities identified from the first phrase."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_entities.head(1)[0].entities[0].entities[0].id)\n",
        "print(df_entities.head(1)[0].entities[0].entities[0].url)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the entities identified from the second phrase."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_entities.tail(1)[0].entities[0].entities[0].id)\n",
        "print(df_entities.tail(1)[0].entities[0].entities[0].url)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a customer recommendation model\n",
        "\n",
        "\n",
        "This notebook uses sample data to train a LightGBM model for retail product recommendation. The data is randomly generated."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from pyspark.version import __version__ as pyspark_version\n",
        "\n",
        "from synapse.ml.core import __spark_package_version__\n",
        "from synapse.ml.train import ComputeModelStatistics\n",
        "from synapse.ml.lightgbm import LightGBMClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "pd.set_option('display.max_columns', 50)\n",
        "\n",
        "print(f\"PySpark version: {pyspark_version}\")\n",
        "print(f\"SynapseML version: {__spark_package_version__}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: If you're using a Managed VNet enabled workspace, please download the dataset from the \n",
        "[url](https://synapsemlpublic.blob.core.windows.net/files/PersonalizedData.csv) and then upload it to your own storage account in order to access it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Blob url\n",
        "# Original blob: \"https://recodatasets.z20.web.core.windows.net/random-dataset/PersonalizedData.csv\"\n",
        "url = \"wasbs://files@synapsemlpublic.blob.core.windows.net/PersonalizedData.csv\"\n",
        "\n",
        "# Data parameters\n",
        "LABEL_COL = \"Rating\"\n",
        "FEATURE_COL = \"features\"\n",
        "RATIO = 0.8\n",
        "SEED = 42\n",
        "\n",
        "# Model parameters\n",
        "OBJECTIVE = \"binary\"\n",
        "BOOSTING = \"gbdt\"\n",
        "NUM_LEAVES = 32\n",
        "NUM_ITERATIONS = 100\n",
        "LEARNING_RATE = 0.1\n",
        "FEATURE_FRACTION = 0.8\n",
        "EARLY_STOPPING_ROUND = 10\n",
        "MODEL_NAME = \"lgb-quickstart\"\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the data from Blob"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Added the file to linked ADLSv2\n",
        "raw_data = spark.read.csv(url, header=True, inferSchema=True)\n",
        "print(\"Schema: \")\n",
        "# raw_data.printSchema()\n",
        "\n",
        "df = raw_data.toPandas()\n",
        "print(\"Shape: \", df.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.iloc[:10, :])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data visualization"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the correlation matrix\n",
        "corr = df.corr()\n",
        "\n",
        "# plot the correlation heatmap\n",
        "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
        "\n",
        "sns.heatmap(corr, \n",
        "            xticklabels=corr.columns, \n",
        "            yticklabels=corr.columns, \n",
        "            cmap='RdBu', \n",
        "            vmin=-1, \n",
        "            vmax=1, \n",
        "            ax=ax, \n",
        "            annot=True,\n",
        "            fmt='.2f', \n",
        "            annot_kws={'size': 10})\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scatterplot\n",
        "sns.set()\n",
        "sns.pairplot(df, height=2.5)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the data into train, test\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train, raw_test = raw_data.randomSplit([RATIO, 1 - RATIO], seed=SEED)\n",
        "print(\"Train: (rows, columns) = {}\".format((raw_train.count(), len(raw_train.columns))))\n",
        "print(\"Test: (rows, columns) = {}\".format((raw_test.count(), len(raw_test.columns))))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering \n",
        "Transform the original data feature columns into feature vectors"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = raw_data.columns[3:]\n",
        "featurizer = VectorAssembler(inputCols=columns, outputCol=FEATURE_COL)\n",
        "train = featurizer.transform(raw_train)[LABEL_COL, FEATURE_COL]\n",
        "test = featurizer.transform(raw_test)[LABEL_COL, FEATURE_COL]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if data is unbalanced\n",
        "display(train.groupBy(LABEL_COL).count())\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm = LightGBMClassifier(\n",
        "    labelCol=LABEL_COL,\n",
        "    featuresCol=FEATURE_COL,\n",
        "    objective=OBJECTIVE,\n",
        "    isUnbalance=False,\n",
        "    boostingType=BOOSTING,\n",
        "    boostFromAverage=True,\n",
        "    baggingSeed=SEED,\n",
        "    numLeaves=NUM_LEAVES,\n",
        "    numIterations=NUM_ITERATIONS,\n",
        "    learningRate=LEARNING_RATE,\n",
        "    featureFraction=FEATURE_FRACTION,\n",
        "    earlyStoppingRound=EARLY_STOPPING_ROUND\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = lgbm.fit(train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importances"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances = model.getFeatureImportances()\n",
        "fi = pd.Series(feature_importances,index = columns)\n",
        "fi = fi.sort_values(ascending = True)\n",
        "f_index = fi.index\n",
        "f_values = fi.values\n",
        " \n",
        "# print feature importances \n",
        "print ('f_index:',f_index)\n",
        "print ('f_values:',f_values)\n",
        "\n",
        "# plot\n",
        "x_index = list(range(len(fi)))\n",
        "x_index = [x/len(fi) for x in x_index]\n",
        "plt.rcParams['figure.figsize'] = (10,10)\n",
        "plt.barh(x_index,f_values,height = 0.028 ,align=\"center\",color = 'tan',tick_label=f_index)\n",
        "plt.xlabel('importances')\n",
        "plt.ylabel('features')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Prediction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(test)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(predictions.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = (\n",
        "    ComputeModelStatistics()\n",
        "    .setScoredLabelsCol(\"prediction\")\n",
        "    .setLabelCol(LABEL_COL)\n",
        "    .setEvaluationMetric(\"classification\")\n",
        ")\n",
        "\n",
        "metrics = evaluator.transform(predictions)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(metrics)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model\n",
        "\n",
        "Save the model to linked ADLS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODEL_NAME)\n",
        "(model\n",
        " .write()\n",
        " .overwrite()\n",
        " .save(MODEL_NAME))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}