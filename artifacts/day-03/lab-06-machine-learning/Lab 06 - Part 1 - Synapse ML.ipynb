{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Install Synapse ML into the Apache Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-05T00:27:50.2395367Z",
              "execution_start_time": "2022-02-05T00:27:50.2391577Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-05T00:23:40.3012327Z",
              "session_id": 11,
              "session_start_time": "2022-02-05T00:23:40.3912316Z",
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1
            },
            "text/plain": [
              "StatementMeta(, 11, -1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"name\": \"synapseml\",\n",
        "  \"conf\": {\n",
        "      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.9.4\",\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12\",\n",
        "      \"spark.yarn.user.classpath.first\": \"true\"\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Detect entities in text using the Cognitive Services entity detector transformer from Synapse ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Retrieve the Cognitive Services credentials and create the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-05T00:32:00.2048518Z",
              "execution_start_time": "2022-02-05T00:32:00.0659913Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-05T00:31:59.8907598Z",
              "session_id": 11,
              "session_start_time": null,
              "spark_pool": "SparkPool02",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(SparkPool02, 11, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "key = mssparkutils.credentials.getSecret('asakeyvault524101', 'CognitiveService')\n",
        "location = 'northeurope'\n",
        "\n",
        "df = spark.createDataFrame(data=[\n",
        "        [1, \"Muad'Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn. It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult. Muad'Dib knew that every experience carries its lesson.\"],\n",
        "        [2, \"It’s the ship that made the Kessel run in less than twelve parsecs. I’ve outrun Imperial starships. Not the local bulk cruisers, mind you. I’m talking about the big Corellian ships, now. She’s fast enough for you, old man.\"]\n",
        "    ], \n",
        "    schema=[\"id\",\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run the transformer and detect the entities mentioned in text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-05T00:32:55.946454Z",
              "execution_start_time": "2022-02-05T00:32:50.6766811Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-05T00:32:50.550337Z",
              "session_id": 11,
              "session_start_time": null,
              "spark_pool": "SparkPool02",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(SparkPool02, 11, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from synapse.ml.cognitive import *\n",
        "\n",
        "entity = (EntityDetector()\n",
        "      .setSubscriptionKey(key)\n",
        "      .setLocation(location)\n",
        "      .setLanguage(\"en\")\n",
        "      .setOutputCol(\"entities\")\n",
        "      .setErrorCol(\"error\"))\n",
        "\n",
        "df_entities = entity.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Check out the entities identified from the first phrase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-05T00:33:30.399129Z",
              "execution_start_time": "2022-02-05T00:32:58.9432877Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-05T00:32:58.8236244Z",
              "session_id": 11,
              "session_start_time": null,
              "spark_pool": "SparkPool02",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(SparkPool02, 11, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "TypeError: 'NoneType' object is not subscriptable",
            "Traceback (most recent call last):\n",
            "TypeError: 'NoneType' object is not subscriptable\n"
          ]
        }
      ],
      "source": [
        "print(df_entities.head(1)[0].entities[0].entities[0].id)\n",
        "print(df_entities.head(1)[0].entities[0].entities[0].url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Check out the entities identified from the second phrase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-01-28T22:08:20.219489Z",
              "execution_start_time": "2022-01-28T22:08:17.4554037Z",
              "livy_statement_state": "available",
              "queued_time": "2022-01-28T22:08:17.3559784Z",
              "session_id": 0,
              "session_start_time": null,
              "spark_pool": "SparkPool02",
              "state": "finished",
              "statement_id": 82
            },
            "text/plain": [
              "StatementMeta(SparkPool02, 0, 82, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Millennium Falcon\n",
            "https://en.wikipedia.org/wiki/Millennium_Falcon"
          ]
        }
      ],
      "source": [
        "print(df_entities.tail(1)[0].entities[0].entities[0].id)\n",
        "print(df_entities.tail(1)[0].entities[0].entities[0].url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Train a customer recommendation model\n",
        "\n",
        "\n",
        "This notebook uses sample data to train a LightGBM model for retail product recommendation. The data is randomly generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"name\": \"synapseml\",\n",
        "  \"conf\": {\n",
        "      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.9.4\",\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12\",\n",
        "      \"spark.yarn.user.classpath.first\": \"true\"\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from pyspark.version import __version__ as pyspark_version\n",
        "\n",
        "from synapse.ml.core import __spark_package_version__\n",
        "from synapse.ml.train import ComputeModelStatistics\n",
        "from synapse.ml.lightgbm import LightGBMClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "pd.set_option('display.max_columns', 50)\n",
        "\n",
        "print(f\"PySpark version: {pyspark_version}\")\n",
        "print(f\"SynapseML version: {__spark_package_version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: If you're using a Managed VNet enabled workspace, please download the dataset from the \n",
        "[url](https://synapsemlpublic.blob.core.windows.net/files/PersonalizedData.csv) and then upload it to your own storage account in order to access it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Blob url\n",
        "# Original blob: \"https://recodatasets.z20.web.core.windows.net/random-dataset/PersonalizedData.csv\"\n",
        "url = \"wasbs://files@synapsemlpublic.blob.core.windows.net/PersonalizedData.csv\"\n",
        "\n",
        "# Data parameters\n",
        "LABEL_COL = \"Rating\"\n",
        "FEATURE_COL = \"features\"\n",
        "RATIO = 0.8\n",
        "SEED = 42\n",
        "\n",
        "# Model parameters\n",
        "OBJECTIVE = \"binary\"\n",
        "BOOSTING = \"gbdt\"\n",
        "NUM_LEAVES = 32\n",
        "NUM_ITERATIONS = 100\n",
        "LEARNING_RATE = 0.1\n",
        "FEATURE_FRACTION = 0.8\n",
        "EARLY_STOPPING_ROUND = 10\n",
        "MODEL_NAME = \"lgb-quickstart\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read the data from Blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Added the file to linked ADLSv2\n",
        "raw_data = spark.read.csv(url, header=True, inferSchema=True)\n",
        "print(\"Schema: \")\n",
        "# raw_data.printSchema()\n",
        "\n",
        "df = raw_data.toPandas()\n",
        "print(\"Shape: \", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(df.iloc[:10, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# calculate the correlation matrix\n",
        "corr = df.corr()\n",
        "\n",
        "# plot the correlation heatmap\n",
        "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
        "\n",
        "sns.heatmap(corr, \n",
        "            xticklabels=corr.columns, \n",
        "            yticklabels=corr.columns, \n",
        "            cmap='RdBu', \n",
        "            vmin=-1, \n",
        "            vmax=1, \n",
        "            ax=ax, \n",
        "            annot=True,\n",
        "            fmt='.2f', \n",
        "            annot_kws={'size': 10})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#scatterplot\n",
        "sns.set()\n",
        "sns.pairplot(df, height=2.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Split the data into train, test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "raw_train, raw_test = raw_data.randomSplit([RATIO, 1 - RATIO], seed=SEED)\n",
        "print(\"Train: (rows, columns) = {}\".format((raw_train.count(), len(raw_train.columns))))\n",
        "print(\"Test: (rows, columns) = {}\".format((raw_test.count(), len(raw_test.columns))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Feature engineering \n",
        "Transform the original data feature columns into feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "columns = raw_data.columns[3:]\n",
        "featurizer = VectorAssembler(inputCols=columns, outputCol=FEATURE_COL)\n",
        "train = featurizer.transform(raw_train)[LABEL_COL, FEATURE_COL]\n",
        "test = featurizer.transform(raw_test)[LABEL_COL, FEATURE_COL]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Check if data is unbalanced\n",
        "display(train.groupBy(LABEL_COL).count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "lgbm = LightGBMClassifier(\n",
        "    labelCol=LABEL_COL,\n",
        "    featuresCol=FEATURE_COL,\n",
        "    objective=OBJECTIVE,\n",
        "    isUnbalance=False,\n",
        "    boostingType=BOOSTING,\n",
        "    boostFromAverage=True,\n",
        "    baggingSeed=SEED,\n",
        "    numLeaves=NUM_LEAVES,\n",
        "    numIterations=NUM_ITERATIONS,\n",
        "    learningRate=LEARNING_RATE,\n",
        "    featureFraction=FEATURE_FRACTION,\n",
        "    earlyStoppingRound=EARLY_STOPPING_ROUND\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model = lgbm.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importances = model.getFeatureImportances()\n",
        "fi = pd.Series(feature_importances,index = columns)\n",
        "fi = fi.sort_values(ascending = True)\n",
        "f_index = fi.index\n",
        "f_values = fi.values\n",
        " \n",
        "# print feature importances \n",
        "print ('f_index:',f_index)\n",
        "print ('f_values:',f_values)\n",
        "\n",
        "# plot\n",
        "x_index = list(range(len(fi)))\n",
        "x_index = [x/len(fi) for x in x_index]\n",
        "plt.rcParams['figure.figsize'] = (10,10)\n",
        "plt.barh(x_index,f_values,height = 0.028 ,align=\"center\",color = 'tan',tick_label=f_index)\n",
        "plt.xlabel('importances')\n",
        "plt.ylabel('features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(predictions.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "evaluator = (\n",
        "    ComputeModelStatistics()\n",
        "    .setScoredLabelsCol(\"prediction\")\n",
        "    .setLabelCol(LABEL_COL)\n",
        "    .setEvaluationMetric(\"classification\")\n",
        ")\n",
        "\n",
        "metrics = evaluator.transform(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "display(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Save the model\n",
        "\n",
        "Save the model to linked ADLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(MODEL_NAME)\n",
        "(model\n",
        " .write()\n",
        " .overwrite()\n",
        " .save(MODEL_NAME))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
